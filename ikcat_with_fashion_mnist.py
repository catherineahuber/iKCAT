# -*- coding: utf-8 -*-
"""iKCAT with Fashion MNIST

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_1k-hmXD3yG9ucIXE4yIc1NTVfBotslp
"""

#Mounting drive
from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score, silhouette_samples
import sklearn as sk
from sklearn.decomposition import PCA
from sklearn.preprocessing import normalize
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC, LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import NearestNeighbors
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix
from statistics import mean
import math
import statistics
from scipy import stats
from sklearn.metrics import pairwise_distances_argmin_min
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import precision_recall_fscore_support

"""#Pre-Processing"""

#reading in Fashion MNIST Dataset from drive (you can replace this path with the path to your own dataset)

df = pd.read_csv("/content/drive/MyDrive/MedIX - Subtype Discovery Project/Data and Analysis/fashion-mnist_train.csv")

#labels for this dataset
y = (df['label']).to_numpy()

#dropping labels so that it doesn't interfere with clustering
X = df.drop(['label'], axis = 1)

#determining the best number of princial components to reduce the dimensions to

!pip install kneed
from kneed import KneeLocator


#
# Instantiate PCA
#
pca = PCA()
#
# Determine transformed features
#
X_pca = pca.fit_transform(X)



#
# Determine explained variance using explained_variance_ration_ attribute
#
exp_var_pca = pca.explained_variance_ratio_

#
# Cumulative sum of eigenvalues; This will be used to create step plot
# for visualizing the variance explained by each principal component.
#
cum_sum_eigenvalues = np.cumsum(exp_var_pca)

#
# Create the visualization plot
#
plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')
plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')

plt.ylabel('Explained variance ratio')
plt.xlabel('Principal component index')
plt.legend(loc='best')
plt.tight_layout()
plt.show()



kn = KneeLocator(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, curve = 'concave', direction = 'increasing')

kn.plot_knee()

print("Best Number of Principal Components: {}".format(kn.knee))

#PCA to reduce the dimensions of Fashion MNIST

pca = PCA(n_components = 76) 
X = pca.fit_transform(X)

"""#Iterative Clustering"""

#creating list of IDs for samples in dataset 
def createList(r1, r2):
    return [item for item in range(r1, r2)]
      
# Driver Code
r1, r2 = 0, len(list(df['label']))
lstOfIds = createList(r1, r2)

#claclualtes the homogeneity per cluster
def calculateHomogeneity(predicted, actual, origNumLabels = 10):
  numPredictedClusters = len(np.unique(predicted)) # number of predicted clusters
  numActual = len(np.unique(actual)) # number of actual ground truth classes

  clusterHomogeneities = []
  clusterTotals = []
  for i in range(numPredictedClusters):
    homogeneityList = np.zeros(origNumLabels)
    # find all of the indices of the samples in each cluster 
    ind = np.where(np.isin(predicted,i))[0]
    actual = np.array(actual)
    actualVals = actual[ind] # finding the actual values in that cluster
    totalNumInCluster = len(actualVals) # finding the amount of points in that cluster
    clusterTotals.append(totalNumInCluster)
    for i in actualVals:
      homogeneityList[i] = homogeneityList[i] + 1
    homogeneityList = homogeneityList / totalNumInCluster
    clusterHomogeneities.append(max(homogeneityList))
  return [clusterHomogeneities, clusterTotals]

#iterative clusterin function; tries to remove at least one good cluster during each iteration
def iterativeClustering(threshold, data, actual, IdList, centers = [], properlyClassified = [], hsOfRemoved = [], lengthOfData = 60000):
 
  print("Number of data points to work with: ", data.shape[0])

  #finding the best k

  k = 1;
  currHomogeneity = 0.0;
  while (currHomogeneity < threshold and k <= math.sqrt(lengthOfData/2)):
    k = k + 1
    #print("K Value: ", k)
    model = sk.cluster.KMeans(n_clusters = k)
    clusterLabels = model.fit_predict(data)
    clusterCenters = model.cluster_centers_ # getting the centers of the clusters
    homogeneityList = calculateHomogeneity(clusterLabels, actual) 
    currHomogeneity = np.max(homogeneityList[0])
  

  predLabels = clusterLabels
  homogeneityPCluster = homogeneityList[0]
  
  for i in range(k):
    print("Homogeneity of cluster {} is {}".format(i,homogeneityPCluster[i]))
  
  #find clusters to remove
  clustersToRemove = []
  bestClusterHS = currHomogeneity
  print("this is max: {}".format(bestClusterHS))
  indexOfMax = homogeneityPCluster.index(bestClusterHS) 
  clusterTotals = homogeneityList[1]
  for i in range(k):
    diffBtwnBest = bestClusterHS - homogeneityPCluster[i]
    
    if i == indexOfMax:
      clustersToRemove.append(i)
      
      print("Cluster sectioned off:", i)
      continue
    if diffBtwnBest < .03 and homogeneityPCluster[i] >= threshold and clusterTotals[i]> math.sqrt(len(data)):
      
      clustersToRemove.append(i)
      
      print("Cluster sectioned off:", i)
    
  #so now we have clusters we want to remove
  #we need to calculate number of dpts in remaining clusters to determine if we stop

  pointsRemoved = 0
  totalPoints = sum(clusterTotals)
  for i in range(k):
    if i in clustersToRemove:
      pointsRemoved += clusterTotals[i] #clusterTotals in total points per cluster

  
  remainingDatapoints = totalPoints - pointsRemoved #check total points
  print("percent removed: {}".format(pointsRemoved/lengthOfData))
  if remainingDatapoints < math.sqrt(lengthOfData) or (pointsRemoved <= math.sqrt(len(data))): #need to check total points; needs to be constant variable; 
    print("DONE - we have all clusters")
    print("properly classified type: {}. Remaining type: {}".format(type(properlyClassified), type(data)))
    return list(properlyClassified), list(IdList), list(hsOfRemoved), list(centers)

  else: 
    #remove these dataPoints in cluster chosen to be separated
    #recluster remaining
    
    IdList = np.array(IdList)

    for num in clustersToRemove:
      indices = np.where(np.isin(predLabels,num))
      toAppend = IdList[indices]
      properlyClassified.append(toAppend)
      centers.append(clusterCenters[num])
      hsOfRemoved.append(homogeneityPCluster[num])
      data = np.delete(data, indices, axis=0)
      actual = np.delete(actual, indices) #axis=0
      IdList = np.delete(IdList,indices)
      predLabels = np.delete(predLabels,indices)
    
    print("Good clusters removed. Starting over with smaller dataset.")
    print("Length of properly classified: ", len(properlyClassified))
    count = 0
    for i in properlyClassified:
      print("Length of cluster {} : {}".format(count, len(i)))
      count = count + 1
    return iterativeClustering(threshold, data, actual,IdList, centers, properlyClassified, hsOfRemoved)

separatedClusters, remainingPoints, scores, centers = iterativeClustering(0.8,X,y,lstOfIds)

"""#Training/Testing classifier with subtypes from iterative clustering, traditional clusters (same number as the number of subtypes we got from iterative clustering), and ground truth classes"""

#finds the ids of the samples in X-text (used to determine number of misclassified points)
def getIds(X_test):
  indexLst = []
  for lst in X_test:
    index = int(lst[-1])
    indexLst.append(index)
  return indexLst

#find points misclassified in each cluster
def findMisclassified(X, X_test, y, y_test,listOfIndices): 
  numMisclassified = []
  indicesInX = listOfIndices #np.where(np.isin(predLabels,num))
  groundTruths = [y[i] for i in indicesInX]
  for i in np.unique(y_test):
    numMisclassifiedPerCluster = 0
    indicesOfInterest = np.where(y_test == i)[0]
    truthLabels = [groundTruths[i] for i in indicesOfInterest]
    majorityClassLabel = int(stats.mode(truthLabels)[0])
    for i in truthLabels:
      if i!=majorityClassLabel:
        numMisclassifiedPerCluster = numMisclassifiedPerCluster + 1
    numMisclassified.append(numMisclassifiedPerCluster)
  return numMisclassified

def redefineXandY():
  df = pd.read_csv("/content/drive/MyDrive/MedIX - Subtype Discovery Project/Data and Analysis/fashion-mnist_train.csv")
  #labels for this dataset
  y = (df['label']).to_numpy()

  #dropping labels so that it doesn't interfere with clustering
  X = df.drop(['label'], axis = 1)
  pca = PCA(n_components = 76)
  X = pca.fit_transform(X)
  return X,y

#creating labels for clusters from iterative clustering

labelsForEverything = np.zeros(60000)

labelForRemaining = len(separatedClusters)

for label in range(len(separatedClusters)):
  for index in separatedClusters[label]:
    labelsForEverything[index] = label


for index in remainingPoints: 
  labelsForEverything[index] = labelForRemaining

indices = np.where(labelsForEverything==labelForRemaining)
print(indices)

copyOfX = X
XRemoved = np.delete(copyOfX,indices[0],axis=0)

copyOfEverything = labelsForEverything
labelsForRemoved = np.delete(copyOfEverything,indices[0])

#Classify MNIST into n classes with iterative clustering subtypes
X,y = redefineXandY()

r1, r2 = 0, len(list(y))
ids = createList(r1, r2)
X = pd.DataFrame(X)
X['ID'] = ids
X = X.to_numpy()
X_train, X_test, y_train, y_test = train_test_split(X, labelsForEverything, test_size = 0.2, stratify = labelsForEverything)
lstOfIndices = getIds(X_test)

X_train1 = []

for i in range(len(X_train)):
  everythingButLast = X_train[i][:-1]
  X_train1.append(everythingButLast)
    
  
X_train1 = np.array(X_train1)

X_test1 = []
for j in range(len(X_test)):
  X_test1.append(X_test[j][:-1])

X_test1 = np.array(X_test1)


svm = SVC()
svm.fit(X_train1, y_train)
svmResults = svm.predict(X_test1)
matrix = confusion_matrix(y_test, svmResults)
accuracies = matrix.diagonal()/matrix.sum(axis = 1)
print("Accuracies per class for our ITERATIVE subtypes: ", accuracies)
print("Overall accuracy for our ITERATIVE subtypes: ", accuracy_score(y_test, svmResults))
print("Recall for our ITERATIVE subtypes: ", recall_score(y_test, svmResults, average = 'macro'))
print("Recall for our ITERATIVE subtypes: ", recall_score(y_test, svmResults, average = None))
print("Precision for our ITERATIVE subtypes: ", precision_score(y_test, svmResults, average = 'macro'))
print("Precision for our ITERATIVE subtypes: ", precision_score(y_test, svmResults, average = None))
print("Number of points Misclassified per cluster: ", findMisclassified(X, X_test1, y, y_test,lstOfIndices))

print(mean([74, 249, 9, 165, 132, 118, 95, 26, 67, 4, 25, 22, 14, 16, 20, 2, 14, 10, 23, 3526])) #with unclustered
print(mean([74, 249, 9, 165, 132, 118, 95, 26, 67, 4, 25, 22, 14, 16, 20, 2, 14, 10, 23])) #without unclustered

# Classify MNIST into ground truth classes (no clustering)
X,y = redefineXandY()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)
svm = SVC()
svm.fit(X_train, y_train)
svmResults = svm.predict(X_test)
# at this point, we should have an accuracy metric
matrix = confusion_matrix(y_test, svmResults)
accuracies = matrix.diagonal()/matrix.sum(axis=1)
print("Accuracies per class for a simple SVM classifer: ", accuracies)
print("Overall accuracy for a simple SVM classifier: ", accuracy_score(y_test, svmResults))
print("Recall for a simple SVM classifier: ", recall_score(y_test, svmResults, average = 'macro'))
print("Precision for a simple SVM classifier: ", precision_score(y_test, svmResults, average = 'macro'))

X,y = redefineXandY()


model = sk.cluster.KMeans(n_clusters = len(separatedClusters))
clusterLabels = model.fit_predict(X)

#classifying MNIST into n classes using traditional k-means clusters



r1, r2 = 0, len(list(y))
ids = createList(r1, r2)
X = pd.DataFrame(X)
X['ID'] = ids
X = X.to_numpy()
X_train, X_test, y_train, y_test = train_test_split(X, clusterLabels, test_size = 0.2, stratify = clusterLabels)
lstOfIndices = getIds(X_test)

X_train1 = []

for i in range(len(X_train)):
  everythingButLast = X_train[i][:-1]
  X_train1.append(everythingButLast)
    
  
X_train1 = np.array(X_train1)

X_test1 = []
for j in range(len(X_test)):
  X_test1.append(X_test[j][:-1])

X_test1 = np.array(X_test1)

svm = SVC()
svm.fit(X_train1, y_train)
svmResults = svm.predict(X_test1)
matrix = confusion_matrix(y_test, svmResults)
accuracies = matrix.diagonal()/matrix.sum(axis = 1)
print("Accuracies per class for our TRADITIONAL subtypes: ", accuracies)
print("Overall accuracy for our TRADITIONAL subtypes: ", accuracy_score(y_test, svmResults))
print("Recall for our TRADITIONAL subtypes: ", recall_score(y_test, svmResults, average = 'macro'))
print("Precision for our TRADITIONAL subtypes: ", precision_score(y_test, svmResults, average = 'macro'))
print("Number of points Misclassified per cluster: ", findMisclassified(X, X_test1, y, y_test,lstOfIndices))

print(mean([1, 515, 221, 323, 59, 674, 386, 19, 116, 262, 13, 353, 373, 87, 19, 112, 329, 79, 350]))

"""#Paper Results
##Class Distributions (Iterative vs Traditional Clustering)
##cluster center pictures
"""

#class distributions for traditional n clusters
separatedClustersTrad = []
numPredictedClusters = len(np.unique(clusterLabels)) # number of predicted clusters
numActual = len(np.unique(y)) # number of actual ground truth classes
labelDictionary = {0:"Tshirt",1:"Trouser",2:"Pullover",3: "Dress",4 : "Coat",5 : "Sandal",6 : "Shirt",7 : "Sneaker",8 : "Bag",9 : "Ankle Boot"}
legend = {0: 'rosybrown', 1:'silver', 2: 'darkred', 3:'coral', 4:'sienna', 5:'peachpuff', 6: 'darkorange', 7:'goldenrod', 8:'darkkhaki', 9:'olive', 10:'darkolivegreen', 11:'lawngreen', 12:'darkseagreen', 13:'aquamarine', 14:'darkslategray', 15:'cyan', 16:'dodgerblue', 17:'cornflowerblue', 18:'indigo', 19:'fuchsia', 20:'palevioletred'}



lst = calculateHomogeneity(clusterLabels,y)
hsPClusterTrad = lst[0]
print("Average homogeneity for TRADITIONAL clustering: ", mean(lst[0]))

plt.figure(1)
for i in range(numPredictedClusters): #0-n clusters
  classCount = {}
  print("this is class distribution for cluster {}".format(i))
  print("this is hs for cluster {}".format(hsPClusterTrad[i]))
  #now we find in pred indexs of points with label i
  ind = np.where(np.isin(clusterLabels,i))
  separatedClustersTrad.append(ind)
  #with those indexes we find actual values for all of those points in cluster
  actualVals = y[ind] #y is actual labels
  #now we iterate through actual to find counts for each class in cluster
  for a in actualVals:
    if a in classCount:
      classCount[a] += 1
    else:
      classCount[a] = 1
  #now we have counts for each class in cluster
  #now we need to report counts and make plots
  classesPerCluster = []
  percentagePClass = []
  for key in classCount:
    percentage = (classCount[key]/sum(classCount.values()))*100
    print("Number of points of class {} in cluster {}: {} ({})".format(labelDictionary[key],i,classCount[key],percentage))
    classesPerCluster.append(labelDictionary[key])
    percentagePClass.append(percentage)

  plt.subplot()
  plt.xticks(rotation=45, rotation_mode='anchor')
  plt.bar(classesPerCluster,percentagePClass,width=.7, align='center',color = legend[i])
  plt.title("Cluster {} (Overclustering)".format(i))
  plt.xlabel("Classes in Cluster {}".format(i))
  plt.ylabel("Percent of Cluster {}".format(i))
  plt.show()

print(hsPClusterTrad)

hsPClusterTrad.sort()
print(hsPClusterTrad)

#Class Distributions for n clusters separated through iterative clustering
clusterIter = 0
labelDictionary = {0:"Tshirt",1:"Trouser",2:"Pullover",3: "Dress",4 : "Coat",5 : "Sandal",6 : "Shirt",7 : "Sneaker",8 : "Bag",9 : "Ankle Boot"}
legend = {0: 'rosybrown', 1:'silver', 2: 'darkred', 3:'coral', 4:'sienna', 5:'peachpuff', 6: 'darkorange', 7:'goldenrod', 8:'darkkhaki', 9:'olive', 10:'darkolivegreen', 11:'lawngreen', 12:'darkseagreen', 13:'aquamarine', 14:'darkslategray', 15:'cyan', 16:'dodgerblue', 17:'cornflowerblue', 18:'indigo', 19:'fuchsia', 20:'palevioletred'}


plt.figure(1)
for cluster in separatedClusters:
  print("this is the class distribution for cluster: {}\n".format(clusterIter))
  print("this is the homogeneity Score for this cluster: {}\n".format(scores[clusterIter]))
  numLabelPerCluster = {}
  for i in cluster:
    label = y[i]
    if label in numLabelPerCluster:
      numLabelPerCluster[label] += 1
    else:
      numLabelPerCluster[label] = 1
  
  classesPerCluster = []
  percentageOfClasses = []
  for key in numLabelPerCluster:
    percentage = (numLabelPerCluster[key]/len(cluster))*100
    print("Number of points of class {} in cluster {}: {} ({})".format(labelDictionary[key],clusterIter,numLabelPerCluster[key],percentage))
    classesPerCluster.append(labelDictionary[key])
    percentageOfClasses.append(percentage)


  plt.subplot()
  plt.xticks(rotation=45, rotation_mode='anchor')
  plt.bar(classesPerCluster, percentageOfClasses,width=.7, align='center',color = legend[clusterIter])
  plt.title("Cluster {} (Iterative Clustering)".format(clusterIter))
  plt.xlabel("Classes in Cluster {}".format(clusterIter))
  plt.ylabel("Percent of Cluster {}".format(clusterIter)) 

  plt.show()

  clusterIter += 1

print(scores)

print(mean([0.916459697448634, 0.8123982837697884, 0.9933485761795885, 0.8003060443764346, 0.8105997210599721, 0.8342399083882049, 0.8185860382392174, 0.8537117903930131, 0.8088235294117647, 0.9450171821305842, 0.8542963885429639, 0.8328611898016998, 0.8748159057437408, 0.8084577114427861, 0.8038897893030794, 0.915068493150685, 0.8173076923076923, 0.8566176470588235, 0.8007662835249042]))

scores.sort()
print(scores)

#Class Distribution for unclustered data from iterative clustering
numLabelsPerClass = {}
for i in remainingPoints:
  label = y[i]
  #print(label)
  if label in numLabelsPerClass:
    numLabelsPerClass[label] += 1
  else:
    numLabelsPerClass[label] = 1

classesInRemaining = []
percentagesPerClassForRemaining = []
for key in numLabelsPerClass:
  percentage = (numLabelsPerClass[key]/len(remainingPoints))*100
  print("Number of points of class {} in remaining: {} ({})".format(labelDictionary[key],numLabelsPerClass[key],percentage))
  classesInRemaining.append(labelDictionary[key])
  percentagesPerClassForRemaining.append(percentage)


plt.bar(classesInRemaining, percentagesPerClassForRemaining,align='center')
plt.title("Unclustered Data Remaining from Iterative Clustering")
plt.xlabel("Classes in Unclustered Data")
plt.ylabel("Percent of Unclustered Data")
plt.xticks(rotation=45, rotation_mode='anchor')

#Calculating homogeneity of unclustered
labels = y


numLabelsPerClass = {}

for i in remainingPoints:
  label = labels[i]
  #print(label)
  if label in numLabelsPerClass:
    numLabelsPerClass[label] += 1
  else:
    numLabelsPerClass[label] = 1

hsPClass = [] #HS per class in unclustered
for key in numLabelsPerClass:
  homogeneity = (numLabelsPerClass[key]/len(remainingPoints))
  hsPClass.append(homogeneity)

hsWoUnclustered = scores
print(scores)
#print(hsWoUnclustered)
hsWUnclustered = hsWoUnclustered
hsWUnclustered.append(max(hsPClass))
print(hsWUnclustered)


print("this is the max HS for the unclustered data: ",max(hsPClass))
print("this is the avg homogeneity w/ unclustered: {}".format(mean(hsWUnclustered)))

#printing cluster center pictures for iterative clustering 
X = df.drop(['label'], axis = 1)
pca = PCA(n_components = 76) 
X = pca.fit_transform(X)
closest, _ = pairwise_distances_argmin_min(centers, X)
df = df.drop(['label'], axis=1)
df = df.to_numpy()
clusterIter = 0
for i in closest:
  print("This is cluster {}'s pic:".format(clusterIter))
  print("This is an {}".format(y[i]))
  #print(df[i])
  plt.imshow(df[i].reshape(28, 28), cmap = plt.cm.binary)
  plt.show()
  clusterIter = clusterIter + 1

#accuracy
print(mean([0.9785553,  0.98372781, 0.99168399, 0.97576531, 0.9595537,  0.9713877,
 0.94,       0.91256831, 0.95762712, 0.82758621, 0.83229814, 0.85815603,
 0.93382353, 0.825,      0.90243902, 0.78082192, 0.85542169, 0.74074074,
 0.92380952]))
#recall
print(mean([0.9785553,  0.98372781, 0.99168399, 0.97576531, 0.9595537,  0.9713877,
 0.94,       0.91256831, 0.95762712, 0.82758621, 0.83229814, 0.85815603,
 0.93382353, 0.825,      0.90243902, 0.78082192, 0.85542169, 0.74074074,
 0.92380952]))
#precision
print(mean([0.98522727, 0.98300074, 0.97446374, 0.98329049, 0.96223776, 0.96861626,
 0.93377483, 0.92265193, 0.94428969, 0.92307692, 0.90540541, 0.90977444,
 0.92028986, 0.94285714, 0.888,      0.91935484, 0.8875,    0.95238095,
 0.94174757]))