# -*- coding: utf-8 -*-
"""iKCAT Algorithm on the KMNIST dataset

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z9hBYLKydwCcVj_M2AcXOqm3AGkNNpC0
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score, silhouette_samples
import sklearn as sk
from sklearn.decomposition import PCA
from sklearn.preprocessing import normalize
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC, LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import NearestNeighbors
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix
from statistics import mean
import math
import statistics
from scipy import stats
from sklearn.metrics import pairwise_distances_argmin_min
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import precision_recall_fscore_support

"""#Pre-Processing"""

from tensorflow import keras
import tensorflow as tf
import tensorflow_datasets as tfds
!pip install tensorflow-datasets
ds = tfds.load(name="kmnist", split="train")
ds_numpy = tfds.as_numpy(ds)  # Convert `tf.data.Dataset` to Python generator

X = []
y = []
for ex in ds_numpy:
  y.append(ex['label'])
  X.append(ex['image'])
X = np.array(X)
y = np.array(y)
X = X[:,:,:,0]
X = X.reshape(X.shape[0], 784)
print(X.shape)

#determining the best number of princial components to reduce the dimensions to
!pip install kneed
from kneed import KneeLocator
pca = PCA()
# Determine transformed features
X_pca = pca.fit_transform(X)
# Determine explained variance using explained_variance_ration_ attribute
exp_var_pca = pca.explained_variance_ratio_
# Cumulative sum of eigenvalues; This will be used to create step plot
# for visualizing the variance explained by each principal component.
cum_sum_eigenvalues = np.cumsum(exp_var_pca)
# Create the visualization plot
plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')
plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')

plt.ylabel('Explained variance ratio')
plt.xlabel('Principal component index')
plt.legend(loc='best')
plt.tight_layout()
plt.show()
kn = KneeLocator(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, curve = 'concave', direction = 'increasing')
kn.plot_knee()
print("Best Number of Principal Components: {}".format(kn.knee))

#PCA to reduce the dimensions of KMNIST
pca = PCA(n_components = 122)
X = pca.fit_transform(X)

"""#Iterative Clustering"""

#creating list of IDs for samples in dataset 
def createList(r1, r2):
    return [item for item in range(r1, r2)]
      
# Driver Code
r1, r2 = 0, len(y)
lstOfIds = createList(r1, r2)

#Calculates homogeneity per cluster
def calculateHomogeneity(predicted, actual, origNumLabels = 10):
  numPredictedClusters = len(np.unique(predicted)) # number of predicted clusters
  numActual = len(np.unique(actual)) # number of actual ground truth classes

  clusterHomogeneities = []
  clusterTotals = []
  for i in range(numPredictedClusters):
    homogeneityList = np.zeros(origNumLabels)
    # find all of the indices of the samples in each cluster 
    ind = np.where(np.isin(predicted,i))[0]
    actual = np.array(actual)
    actualVals = actual[ind] # finding the actual values in that cluster
    totalNumInCluster = len(actualVals) # finding the amount of points in that cluster
    clusterTotals.append(totalNumInCluster)
    for i in actualVals:
      homogeneityList[i] = homogeneityList[i] + 1
    homogeneityList = homogeneityList / totalNumInCluster
    clusterHomogeneities.append(max(homogeneityList))
  return [clusterHomogeneities, clusterTotals]

def iterativeClustering(threshold, data, actual, IdList, centers = [], properlyClassified = [], hsOfRemoved = [], lengthOfData = 60000):
 
  print("Number of data points to work with: ", data.shape[0])

  #finding the best k

  k = 1;
  currHomogeneity = 0.0;
  while (currHomogeneity < threshold and k <= math.sqrt(lengthOfData/2)):
    k = k + 1
    model = sk.cluster.KMeans(n_clusters = k)
    clusterLabels = model.fit_predict(data)
    clusterCenters = model.cluster_centers_ # getting the centers of the clusters
    homogeneityList = calculateHomogeneity(clusterLabels, actual) 
    currHomogeneity = np.max(homogeneityList[0])
  

  predLabels = clusterLabels
  homogeneityPCluster = homogeneityList[0]
  
  for i in range(k):
    print("Homogeneity of cluster {} is {}".format(i,homogeneityPCluster[i]))
  
  #find clusters to remove
  clustersToRemove = []
  bestClusterHS = currHomogeneity
  print("this is max: {}".format(bestClusterHS))
  indexOfMax = homogeneityPCluster.index(bestClusterHS) 
  clusterTotals = homogeneityList[1]
  for i in range(k):
    diffBtwnBest = bestClusterHS - homogeneityPCluster[i]
    
    if i == indexOfMax:
      clustersToRemove.append(i)
      
      print("Cluster sectioned off:", i)
      continue
    if diffBtwnBest < .03 and homogeneityPCluster[i] >= threshold and clusterTotals[i]> math.sqrt(len(data)):
      
      clustersToRemove.append(i)
      
      print("Cluster sectioned off:", i)

  pointsRemoved = 0
  totalPoints = sum(clusterTotals)
  for i in range(k):
    if i in clustersToRemove:
      pointsRemoved += clusterTotals[i] 

  
  remainingDatapoints = totalPoints - pointsRemoved #check total points
  print("percent removed: {}".format(pointsRemoved/lengthOfData))
  if remainingDatapoints < math.sqrt(lengthOfData) or (pointsRemoved <= 0.01*lengthOfData):
    print("DONE - we have all clusters")
    print("properly classified type: {}. Remaining type: {}".format(type(properlyClassified), type(data)))
    return list(properlyClassified), list(IdList), list(hsOfRemoved), list(centers)

  else: 
    #remove these dataPoints in cluster chosen to be separated
    #recluster remaining
    IdList = np.array(IdList)

    for num in clustersToRemove:
      indices = np.where(np.isin(predLabels,num))
      toAppend = IdList[indices]
      properlyClassified.append(toAppend)
      centers.append(clusterCenters[num])
      hsOfRemoved.append(homogeneityPCluster[num])
      data = np.delete(data, indices, axis=0)
      actual = np.delete(actual, indices)
      IdList = np.delete(IdList,indices)
      predLabels = np.delete(predLabels,indices)
    
    print("Good clusters removed. Starting over with smaller dataset.")
    print("Length of properly classified: ", len(properlyClassified))
    count = 0
    for i in properlyClassified:
      print("Length of cluster {} : {}".format(count, len(i)))
      count = count + 1
    return iterativeClustering(threshold, data, actual,IdList, centers, properlyClassified, hsOfRemoved)

separatedClusters, remainingPoints, scores, centers = iterativeClustering(0.8,X,y,lstOfIds)

mean(scores)

"""#Training/Testing classifier with subtypes from iterative clustering, traditional clusters (same number as the number of subtypes we got from iterative clustering), and ground truth classes"""

#finds the ids of the samples in X-text (used to determine number of misclassified points)
def getIds(X_test):
  indexLst = []
  for lst in X_test:
    index = int(lst[-1])
    indexLst.append(index)
  return indexLst

#find points misclassified in each cluster
def findMisclassified(X, X_test, y, y_test,listOfIndices): 
  numMisclassified = []
  indicesInX = listOfIndices #np.where(np.isin(predLabels,num))
  groundTruths = [y[i] for i in indicesInX]
  for i in np.unique(y_test):
    numMisclassifiedPerCluster = 0
    indicesOfInterest = np.where(y_test == i)[0]
    truthLabels = [groundTruths[i] for i in indicesOfInterest]
    majorityClassLabel = int(stats.mode(truthLabels)[0])
    for i in truthLabels:
      if i!=majorityClassLabel:
        numMisclassifiedPerCluster = numMisclassifiedPerCluster + 1
    numMisclassified.append(numMisclassifiedPerCluster)
  return numMisclassified

#creating labels for clusters from iterative clustering

labelsForEverything = np.zeros(60000)

labelForRemaining = len(separatedClusters)

for label in range(len(separatedClusters)):
  for index in separatedClusters[label]:
    labelsForEverything[index] = label


for index in remainingPoints: 
  labelsForEverything[index] = labelForRemaining

indices = np.where(labelsForEverything==labelForRemaining)
print(indices)

copyOfX = X
XRemoved = np.delete(copyOfX,indices[0],axis=0)

copyOfEverything = labelsForEverything
labelsForRemoved = np.delete(copyOfEverything,indices[0])

#Classify MNIST into n classes with iterative clustering subtypes
r1, r2 = 0, len(list(y))
ids = createList(r1, r2)
X1 = pd.DataFrame(X)
X1['ID'] = ids
X1 = X1.to_numpy()
X_train, X_test, y_train, y_test = train_test_split(X1, labelsForEverything, test_size = 0.2, stratify = labelsForEverything)
lstOfIndices = getIds(X_test)

X_train1 = []

for i in range(len(X_train)):
  everythingButLast = X_train[i][:-1]
  X_train1.append(everythingButLast)
    
  
X_train1 = np.array(X_train1)

X_test1 = []
for j in range(len(X_test)):
  X_test1.append(X_test[j][:-1])

X_test1 = np.array(X_test1)


svm = SVC()
svm.fit(X_train1, y_train)
svmResults = svm.predict(X_test1)
matrix = confusion_matrix(y_test, svmResults)
accuracies = matrix.diagonal()/matrix.sum(axis = 1)
print("Recall per class for our ITERATIVE subtypes: ", recall_score(y_test, svmResults, average = None))
print("Precision per class for our ITERATIVE subtypes: ", precision_score(y_test, svmResults, average = None))
print("Accuracies per class for our ITERATIVE subtypes: ", accuracies)
print("Number of points misclassified per cluster for our ITERATIVE subtypes: ", findMisclassified(X1, X_test1, y, y_test,lstOfIndices))
print("Overall accuracy for our ITERATIVE subtypes: ", accuracy_score(y_test, svmResults))
print("Recall for our ITERATIVE subtypes: ", recall_score(y_test, svmResults, average = 'macro'))
print("Precision for our ITERATIVE subtypes: ", precision_score(y_test, svmResults, average = 'macro'))
print("Number of points Misclassified per cluster: ", mean(findMisclassified(X1, X_test1, y, y_test,lstOfIndices)))

print("Accuracy for - unclustered: ", mean([0.94628571, 0.96088435, 0.96844181, 0.95913462, 0.95810811, 0.92307692,
 0.91363636, 0.92641509, 0.8705036,  0.89473684, 0.9017094,  0.89928058,
 0.87421384, 0.88764045, 0.79527559, 0.904, 0.91842105, 0.95463456]))
print("Precision for - unclustered: ", mean([0.97297297, 0.94166667, 0.95247333, 0.96610169, 0.95810811, 0.95846645,
 0.92626728, 0.95711501, 0.94285714, 0.96356275, 0.95045045, 0.93984962,
 0.96527778, 0.95951417, 0.9266055,  0.8968254,  0.93817204, 0.91028037]))
print("Recall for - unclustered: ", mean([0.94628571, 0.96088435, 0.96844181, 0.95913462, 0.95810811, 0.92307692,
 0.91363636, 0.92641509, 0.8705036,  0.89473684, 0.9017094,  0.89928058,
 0.87421384, 0.88764045, 0.79527559, 0.904, 0.91842105]))
print("Misclassified for - unclustered: ", mean([145, 233, 89, 136, 127, 73, 40, 105, 58, 13, 33, 66, 15, 52, 20, 4, 75]))

print(len(separatedClusters))

# Classify MNIST into ground truth classes (no clustering)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)
svm = SVC()
svm.fit(X_train, y_train)
svmResults = svm.predict(X_test)
# at this point, we should have an accuracy metric
matrix = confusion_matrix(y_test, svmResults)
accuracies = matrix.diagonal()/matrix.sum(axis=1)
print("Accuracies per class for a simple SVM classifer: ", accuracies)
print("Overall accuracy for a simple SVM classifier: ", accuracy_score(y_test, svmResults))
print("Recall for a simple SVM classifier: ", recall_score(y_test, svmResults, average = 'macro'))
print("Precision for a simple SVM classifier: ", precision_score(y_test, svmResults, average = 'macro'))

#classifying MNIST into n classes using traditional k-means clusters

model = sk.cluster.KMeans(n_clusters = len(separatedClusters))
clusterLabels = model.fit_predict(X)
centersForTrad = model.cluster_centers_


r1, r2 = 0, len(list(y))
ids = createList(r1, r2)
X1 = pd.DataFrame(X)
X1['ID'] = ids
X1 = X1.to_numpy()
X_train, X_test, y_train, y_test = train_test_split(X1, clusterLabels, test_size = 0.2, stratify = clusterLabels)
lstOfIndices = getIds(X_test)

X_train1 = []

for i in range(len(X_train)):
  everythingButLast = X_train[i][:-1]
  X_train1.append(everythingButLast)
    
  
X_train1 = np.array(X_train1)

X_test1 = []
for j in range(len(X_test)):
  X_test1.append(X_test[j][:-1])

X_test1 = np.array(X_test1)

svm = SVC()
svm.fit(X_train1, y_train)
svmResults = svm.predict(X_test1)
matrix = confusion_matrix(y_test, svmResults)
accuracies = matrix.diagonal()/matrix.sum(axis = 1)
print("Accuracies per class for our TRADITIONAL subtypes: ", accuracies)
print("Recall per class for our TRADITIONAL subtypes: ", recall_score(y_test, svmResults, average = None))
print("Precision per class for our TRADITIONAL subtypes: ", precision_score(y_test, svmResults, average = None))
print("Overall accuracy for our TRADITIONAL subtypes: ", accuracy_score(y_test, svmResults))
print("Recall for our TRADITIONAL subtypes: ", recall_score(y_test, svmResults, average = 'macro'))
print("Precision for our TRADITIONAL subtypes: ", precision_score(y_test, svmResults, average = 'macro'))
print("Number of points Misclassified per cluster FOR TRADITIONAL: ", mean(findMisclassified(X1, X_test1, y, y_test,lstOfIndices)))

"""#Paper Results
##Class Distributions (Iterative vs Traditional Clustering)
##cluster center pictures
"""

#Calculating homogeneity of unclustered
labels = y


numLabelsPerClass = {}

for i in remainingPoints:
  label = labels[i]
  #print(label)
  if label in numLabelsPerClass:
    numLabelsPerClass[label] += 1
  else:
    numLabelsPerClass[label] = 1

hsPClass = [] #HS per class in unclustered
for key in numLabelsPerClass:
  homogeneity = (numLabelsPerClass[key]/len(remainingPoints))
  hsPClass.append(homogeneity)

hsWoUnclustered = scores
print(scores)
#print(hsWoUnclustered)
hsWUnclustered = hsWoUnclustered
hsWUnclustered.append(max(hsPClass))
print(hsWUnclustered)


print("this is the max HS for the unclustered data: ",max(hsPClass))
print("this is the avg homogeneity w/ unclustered: {}".format(mean(hsWUnclustered)))
print("this is the avg homogeneity w/o unclustered {}".format(mean(scores)))